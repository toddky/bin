#!/usr/bin/env python3
# vim: ft=python ts=4 sw=4 et

# ==============================================================================
# IMPORT
# ==============================================================================
import argparse
import json
import os
import requests
import shutil
import subprocess
import sys

from pathlib import Path
from functools import cache


# ==============================================================================
# FUNCTIONS
# ==============================================================================
@cache
def get_url():
    url_file = Path.home() / '.ai_url'
    url = url_file.read_text().strip()
    return url

@cache
def get_api_key():
    api_key_file = Path.home() / '.ai_key'
    api_key = api_key_file.read_text().strip()
    return api_key

def post(endpoint, **kwargs):
    base_url = get_url()
    key = get_api_key()
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {key}"
    }
    url = f"{base_url}/{endpoint}"
    response = requests.post(url, headers=headers, **kwargs)
    return response

def get(endpoint, **kwargs):
    base_url = get_url()
    key = get_api_key()
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {key}"
    }
    url = f"{base_url}/{endpoint}"
    response = requests.get(url, headers=headers, **kwargs)
    return response

def ask(question, system_prompt, model, stream=True):

    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role": "user",
            "content": question,
        },
    ]
    data = {
        "model": model,
        "messages": messages,
    }
    data["stream"] = stream

    try:
        timeout = 120
        response = post("chat/completions", json=data, stream=stream, timeout=timeout)
    except requests.exceptions.RequestException as e:
        print(f"ERROR: {e}", file=sys.stderr)
        sys.exit(1)

    # Print response for non-streaming mode
    if not stream:
        chunk = response.json()
        print_chunk(chunk)
        print()
        return

    # Print response for streaming mode
    for line in response.iter_lines():
        if not line: continue
        line = line.decode('utf-8')

        if not line.startswith('data: '): continue
        json_str = line[6:]

        # Debug print
        #print(f"\033[90m{json_str}\033[0m", end='')

        if json_str == '[DONE]':
            print()

        try:
            chunk = json.loads(json_str)
            if chunk['choices'][0]['delta'].get('content'):
                print(chunk['choices'][0]['delta']['content'], end='', flush=True)
        except json.JSONDecodeError:
            print(f"\033[90m{json_str}\033[0m", end='')
            continue

def print_chunk(chunk):

    # Check response
    #if 'choices' not in chunk:
    #    error_msg = chunk
    #    if 'error' in chunk:
    #        error_msg = chunk['error']['message']['error']
    #    raise Exception(error_msg)

    choice = chunk['choices'][0]

    if 'delta' in choice:
        content = choice['delta']['content']
    elif 'message' in choice:
        content = choice['message']['content']
    else:
        raise Exception("Unknown chunk format")
    print(content, end='', flush=True)

def list_models():
    response = get("models", timeout=5)
    models = response.json()
    for model in models['data']:
        print(f"- {model['id']}" )


# ==============================================================================
# MAIN
# ==============================================================================
if __name__ == "__main__":

    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Ask a question to the AI.')
    parser.add_argument('--model', default='azure/gpt-4o', help='AI model to use (default: azure/gpt-4o)')
    parser.add_argument('--system-prompt', default='You are a helpful assistant.', help='System prompt to use (default: "You are a helpful assistant.")')
    parser.add_argument('--streaming', action='store_true', help='Enable streaming mode (default: False)')
    parser.add_argument('--list', action='store_true', help='List available models and exit')
    parser.add_argument('question', nargs='*', help='Question to ask the AI. Use "-" to read from stdin.')
    args = parser.parse_args()

    if args.list:
        list_models()
        sys.exit(0)

    # Get question from command line or stdin
    question = None
    if args.question == ['-']:
        question = sys.stdin.read().strip()
    elif args.question:
        question = ' '.join(args.question)

    if not question:
        print("Please provide a question as an argument or use '-' to read from stdin.", file=sys.stderr)
        sys.exit(1)

    # Ask question and get answer
    ask(question, args.system_prompt, args.model, args.streaming)

